{{- if .Values.alerting.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ .Values.name }}
  labels:
{{ include "axoom.default-labels" . | indent 4 }}
    prometheus: cluster
    role: alert-rules

spec:
  groups:
    - name: {{ .Values.name }}.rules
      rules:
        - alert: ReplicasUnavailable
          expr: kube_deployment_status_replicas_unavailable{deployment="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"} > 0
          for: 5m
          labels:
            component: {{ .Values.name }}
            severity: page
{{ if .Values.alerting.labels }}{{ toYaml .Values.alerting.labels | indent 12 }}{{ end }}
          annotations:
            summary: {{ .Values.name }} pods are unavailable
            description: Some of the {{ .Values.name }} pods were unavailable for the last 5 minutes

    - name: {{ .Values.name }}.rules
      rules:
        - alert: HighMemoryUsage
          expr: >
            max(container_memory_working_set_bytes{container_name="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"}) /
            min(kube_pod_container_resource_limits_memory_bytes{container="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"}) >
            {{ .Values.alerting.memoryUsage.thresholdFactor }}
          for: 5m
          labels:
            component: {{ .Values.name }}
            severity: ticket
{{ if .Values.alerting.labels }}{{ toYaml .Values.alerting.labels | indent 12 }}{{ end }}
          annotations:
            summary: {{ .Values.name }} memory usage is high
            description: {{ .Values.name }} is using more than {{ .Values.alerting.memoryUsage.thresholdFactor }} of its available memory

{{- if and .Values.ingress.enabled (not (eq .Values.ingress.class "cluster")) }}
        - alert: Http5xxOccurred
          expr: sum(increase(traefik_backend_requests_total{backend="{{ .Values.ingress.domain }}",code=~"5.."}[1m])) > 0
          labels:
            component: {{ .Values.name }}
            severity: page
{{ if .Values.alerting.labels }}{{ toYaml .Values.alerting.labels | indent 12 }}{{ end }}
          annotations:
            summary: {{ .Values.name }} HTTP 5xx responses occurred
            description: {{ .Values.name }} HTTP 5xx responses occurred in the last minute

        - alert: HighHttp4xxRatio
          expr: >
            (sum(rate(traefik_backend_requests_total{backend="{{ .Values.ingress.domain }}",code=~"4.."}[{{ .Values.alerting.http4xxRatio.sampleInterval }}])) / sum(rate(traefik_backend_requests_total{backend="{{ .Values.ingress.domain }}"}[{{ .Values.alerting.http4xxRatio.sampleInterval }}]))) /
            (sum(rate(traefik_backend_requests_total{backend="{{ .Values.ingress.domain }}",code=~"4.."}[{{ .Values.alerting.http4xxRatio.referenceInterval }}])) / sum(rate(traefik_backend_requests_total{backend="{{ .Values.ingress.domain }}"}[{{ .Values.alerting.http4xxRatio.referenceInterval }}]))) >
            {{ .Values.alerting.http4xxRatio.thresholdFactor }}
          labels:
            component: {{ .Values.name }}
            severity: ticket
{{ if .Values.alerting.labels }}{{ toYaml .Values.alerting.labels | indent 12 }}{{ end }}
          annotations:
            summary: Ratio of {{ .Values.name }} HTTP 4xx responses is high
            description: Ratio of {{ .Values.name }} HTTP responses with 4xx status codes in the last {{ .Values.alerting.http4xxRatio.sampleInterval }} is higher than in the last {{ .Values.alerting.http4xxRatio.referenceInterval }}

        - alert: SlowResponseTime
          expr: >
            (sum(increase(traefik_backend_request_duration_seconds_sum{backend="{{ .Values.ingress.domain }}"}[{{ .Values.alerting.responseTime.sampleInterval }}])) / sum(increase(traefik_backend_requests_total{backend="{{ .Values.ingress.domain }}"}[{{ .Values.alerting.responseTime.sampleInterval }}]))) /
            (sum(increase(traefik_backend_request_duration_seconds_sum{backend="{{ .Values.ingress.domain }}"}[{{ .Values.alerting.responseTime.referenceInterval }}])) / sum(increase(traefik_backend_requests_total{backend="{{ .Values.ingress.domain }}"}[{{ .Values.alerting.responseTime.referenceInterval }}]))) >
            {{ .Values.alerting.responseTime.thresholdFactor }}
          labels:
            component: {{ .Values.name }}
            severity: ticket
{{ if .Values.alerting.labels }}{{ toYaml .Values.alerting.labels | indent 12 }}{{ end }}
          annotations:
            summary: Slow {{ .Values.name }} HTTP repsonses
            description: {{ .Values.name }} HTTP responses in the last {{ .Values.alerting.responseTime.sampleInterval }} were slower than in the last {{ .Values.alerting.responseTime.referenceInterval }}
{{- end }}

{{- end }}
