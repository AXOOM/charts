{{- if .Values.alerting.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ .Values.name }}
  labels:
{{ include "axoom.default-labels" . | indent 4 }}
    prometheus: cluster
    role: alert-rules

spec:
  groups:
    - name: {{ .Values.name }}.rules
      rules:
        - alert: SomeReplicasUnavailable
          expr: kube_deployment_status_replicas_unavailable{deployment="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"} > 0
          for: 10m
          labels:
            component: {{ .Values.name }}
            severity: page
{{- if .Values.alerting.labels }}
{{ toYaml .Values.alerting.labels | indent 12 }}
{{- end }}
          annotations:
            summary: Some {{ .Values.name }} pods are unavailable
            description: Some of the {{ .Values.name }} pods were unavailable for the last 10 minutes

        - alert: AllReplicasUnavailable
          expr: kube_deployment_status_replicas_available{deployment="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"} == 0
          for: 1m
          labels:
            component: {{ .Values.name }}
            severity: page
{{- if .Values.alerting.labels }}
{{ toYaml .Values.alerting.labels | indent 12 }}
{{- end }}
          annotations:
            summary: All {{ .Values.name }} pods are unavailable
            description: All of the {{ .Values.name }} pods were unavailable for the last 1 minute

        - alert: HighMemoryUsage
          expr: >
            max(container_memory_working_set_bytes{container_name="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"}) /
            min(kube_pod_container_resource_limits_memory_bytes{container="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"}) >
            {{ .Values.alerting.memoryUsage.thresholdFactor }}
          for: 5m
          labels:
            component: {{ .Values.name }}
            severity: page
{{- if .Values.alerting.labels }}
{{ toYaml .Values.alerting.labels | indent 12 }}
{{- end }}
          annotations:
            summary: {{ .Values.name }} memory usage is high
            description: {{ .Values.name }} is using more than {{ .Values.alerting.memoryUsage.thresholdFactor }} of its available memory

{{ if and .Values.ingress.enabled (or .Values.ingress.domain .Values.ingress.istio.enabled) }}
        - alert: Http5xxOccurred
          expr: >
            sum(increase({{ include "requests-total" . }}{ {{ include "requests-filter" . }}, {{ include "response-code" . }}=~"5.." }[1m])) > 0
          labels:
            component: {{ .Values.name }}
            severity: page
{{- if .Values.alerting.labels }}
{{ toYaml .Values.alerting.labels | indent 12 }}
{{- end }}
          annotations:
            summary: {{ .Values.name }} HTTP 5xx responses occurred
            description: {{ .Values.name }} HTTP 5xx responses occurred in the last minute

        - alert: HighHttp4xxRatio
          expr: >
            sum(rate({{ include "requests-total" . }}{ {{ include "requests-filter" . }} }[{{ .Values.alerting.http4xxRatio.sampleInterval }}])) > 0 and
            (sum(rate({{ include "requests-total" . }}{ {{ include "requests-filter" . }}, {{ include "response-code" . }}=~"4.." }[{{ .Values.alerting.http4xxRatio.sampleInterval }}])) / sum(rate({{ include "requests-total" . }}{ {{ include "requests-filter" . }} }[{{ .Values.alerting.http4xxRatio.sampleInterval }}]))) /
            (sum(rate({{ include "requests-total" . }}{ {{ include "requests-filter" . }}, {{ include "response-code" . }}=~"4.." }[{{ .Values.alerting.http4xxRatio.referenceInterval }}])) / sum(rate({{ include "requests-total" . }}{ {{ include "requests-filter" . }} }[{{ .Values.alerting.http4xxRatio.referenceInterval }}]))) >
            {{ .Values.alerting.http4xxRatio.thresholdFactor }}
          labels:
            component: {{ .Values.name }}
            severity: ticket
{{- if .Values.alerting.labels }}
{{ toYaml .Values.alerting.labels | indent 12 }}
{{- end }}
          annotations:
            summary: Ratio of {{ .Values.name }} HTTP 4xx responses is high
            description: Ratio of {{ .Values.name }} HTTP responses with 4xx status codes in the last {{ .Values.alerting.http4xxRatio.sampleInterval }} is higher than in the last {{ .Values.alerting.http4xxRatio.referenceInterval }}

        - alert: SlowResponseTime
          expr: >
            sum(increase({{ include "requests-total" . }}{ {{ include "requests-filter" . }} }[{{ .Values.alerting.responseTime.sampleInterval }}])) > 0 and
            (sum(increase({{ include "request-duration" . }}{ {{ include "requests-filter" . }} }[{{ .Values.alerting.responseTime.sampleInterval }}])) / sum(increase({{ include "requests-total" . }}{ {{ include "requests-filter" . }} }[{{ .Values.alerting.responseTime.sampleInterval }}]))) /
            (sum(increase({{ include "request-duration" . }}{ {{ include "requests-filter" . }} }[{{ .Values.alerting.responseTime.referenceInterval }}])) / sum(increase({{ include "requests-total" . }}{ {{ include "requests-filter" . }} }[{{ .Values.alerting.responseTime.referenceInterval }}]))) >
            {{ .Values.alerting.responseTime.thresholdFactor }}
          labels:
            component: {{ .Values.name }}
            severity: ticket
{{- if .Values.alerting.labels }}
{{ toYaml .Values.alerting.labels | indent 12 }}
{{- end }}
          annotations:
            summary: Slow {{ .Values.name }} HTTP repsonses
            description: {{ .Values.name }} HTTP responses in the last {{ .Values.alerting.responseTime.sampleInterval }} were slower than in the last {{ .Values.alerting.responseTime.referenceInterval }}
{{- end }}

{{- end }}
